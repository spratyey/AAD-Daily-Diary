### Algorithm Analysis and Design
### Pratyay Suvarnapathaki, 2020111016
# Week 2, Lecture 1

---

## Problem-Solving
Computational problems, based on their solvability, can be broadly classified under two categories:
1. **Tractable** Problems: Solvable using a polynomial-time algorithm. Naturally, there exists a polynomial upper bound.
2. **Intractable** Problems: Not solvable using a polynomial time algorithm. The *lower* bound is exponential.

To carefully illustrate the methodology of solving tractable problems, we now take two examples, approaching each in multiple ways and comparing the solutions.

---

## The Fibonacci Problem
The problem statement is quite simple.\
Given $n$, the $n^{th}$ Fibonacci number is to be found.

###	Approach 1: Naive Recursion
Pseudocode:
```py
def fib(n):
	if(n<2):
		return n
	else:
		return fib(n-1)+fib(n-2)
```
Provided that enough memory is available, this solution is clearly correct.\
However, the complexity here is: $T(n) = T(n-1) + T(n-2)$ (exponential).\
The natural question now is, **Can we do better?**

### Approach 2: Dynamic Memoisation
Caching the already-computed values reduces computational redundancy, thus improving efficiency.\
Pseudocode:
```py
def fib(n):
	f = [1,2]
	for i in range(2,n):
    	f.append(f[i-1]+f[i-2])
	return f
```
Although here the complexity appears to be $O(n)$, it is in fact $O(n^2)$ owing to the fact that addition of large numbers itself is am $O(n)$ procedure.

### Approach 3: Matrix Multiplication
$
\begin{bmatrix}
F_{N} & F_{N-1}
\end{bmatrix}
=\begin{bmatrix}
F_{0} & F_{1}
\end{bmatrix} \cdot
\begin{bmatrix}
0&1\\
1&1\\
\end{bmatrix}^n
$\
Now, since we don't have to compute for *all* numbers from 1 to n, the complexity is further improved to: $O(M(n)logn)$ where $M(n)$ is the time taken for matrix multiplication for matrices with large entries.

### Approach 4: Mathematical Formula
Using the relation:\
$F_N  =$  $\frac{1}{\sqrt{5}}(\frac{1 + \sqrt{5}}{2})^n - \frac{1}{\sqrt{5}}(\frac{1 - \sqrt{5}}{2})^n$\
This method has complexity $O(n log^2 n)$.\
However, there is still a problem. Precision issues involving the storage of irrational numbers might arise, the fact that we are multiplying $\sqrt{5}$ makes this problem even more prominent.

---

## The Integer Multiplication Problem
Two large numbers expressed in binary form are to be multiplied as efficiently as possible.

### Approach 1: 'The School Way'
Lay out the two numbers in their binary form, then multiply all digits of the first number with the LSB of the second number, then with the Least-but-one-SB, and so on...\
Intuitively, the complexity is $O(n^2)$ for two n-bit numbers.\
The natural question once again is: **Can we do better?**

### Approach 2: The Karatsuba Algorithm
The three multiplications technique:\
Suppose we have to multiply two complex numbers $(a+ib)(c+id)$. As multiplication is costlier than addition/subtraction, we aim to reduce the required 4 multiplicative operations down to 3 in the following way:\
Calculate $ac$ and $bd$\
Calculate $(a+b)(c+d)$ and then get $ad+bc = a+b)(c+d) - ac - bd$\
Thus we have the final answer $(a + ib)(c + id) = ac âˆ’ bd + i(bc + ad)$ using just 3 real number multiplication operations.

We use a similar divide-and-conquer methodology to break large numbers down into smaller numbers and then multiply those.\
Steps:
1. Divide the two n-bit numbers $A$ and $B$ each into 2 integers, each with almost the name number of bits.
2. These are analogous to the real and imaginary parts of the two complex numbers in the above example.
3. Using the three multiplications technique, calculate the product (substitute $i$ with $2^{\frac{n}{2}}$).
4. Do addition and subtraction followed by a $\frac{n}{2}$ place shift to obtain the final product.

This divide and conquer method can be repeated again and again for optimum efficiency.\
Complexity:  $T(n) = 3T(\frac{n}{2}) +$ time taken to add/subtract/shift.\
Using the master theorem this can be improved to $O(n^{\log_2 3}) \approx O(n^{1.585})$\
An even faster method is the Fast Fourier Transform which has complexity $O(n logn log(logn))$\
The best known method, however, has complexity $O(nlogn)$.\
Still, none of these are the *best*, as it is always possible to imrpove the existing paradigms.

---