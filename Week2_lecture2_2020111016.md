### Algorithm Analysis and Design
### Pratyay Suvarnapathaki, 2020111016
# Week 2, Lecture 2

---

## Master Theorem
Used to find the time complexity of the alogorithm when we know hoe the subproblems are divided and combined (as is the case in the *divide-and-conquer* methodology), using a recurrence relation.\
Thus, given the recurrence relation:\
$T(n) = aT(\lceil\frac{n}{b}\rceil) + O(n^d)$\,
The time complexity of the algorithm is:\
$T(n) = \begin{cases} 
O(n^d) & d > \log_b a \\ O(n^d \log_bn) & d = \log_b a \\ O(n^{\log_b a}) & d <\log_b a \end{cases}$

The recursion system forms a sort of tree wherein the work done at level $k$ is $a^k \times O(\frac{n}{b^k})^d = O(n^d) \times (\frac{a}{b^d})^k$

Total work = summation over all levels. This is a GP with $r=\frac{a}{b^d}$ 

Three cases:
1. Ratio is <1 => First term is dominant => $O(n^d)$
2. Ratio = 1 => All $log_b n$ levels have work $O(n^d)$ => $O(n^d \log_bn)$.
3. Ratio is >1 => Last term is dominant => $O(n^{\log_b a})$

---
## Merge-Sort

Procedure:
1.  Split the array into two (almost) equal halves.
2.  Recursively sort the two halves.
3.  Merge the two.

Recurrence relation: $T(n) = 2T(n/2) + O(n)$\
By Master Theorem, complexity = $O(n\log{n})$

Pseudocode for recursive approach:
```py
def merge_sort(list m): 

    if length of m <= 1:
        return m

    leftlist = rightlist = empty list
    for all i: m[i] exists, do:
        if i < (length of m)/2 then
            add m[i] to leftlist
        else
            add m[i] to rightlist

    leftlist = merge_sort(leftlist)
    rightlist = merge_sort(rightlist)

    return merge(left, right)
```
Merge Sort is a stable algorithm, and is optimal because $\Omega(n\log n)$ is the lower bound for comparison based sorting.

---

## Matrix Multiplication

Summary of the complexities of known algorithms:
- Naive: $O(n^3)$
- Strassen's method: $O(n^{\log_2 7})$
- Fastest known: $O(n^{2.37})$
- Optimum: Unknown

### Divide and Conquer
Considering $X$ and $Y$ to be two $n \times n$ matrices, we divide each into four $\frac{n}{2} \times \frac{n}{2}$ parts. We then multiply corresponding subparts to get the subparts of the product matrix.\
Recurrence relation: $T(n) = 8T(\frac{n}{2}) + O(n^2)$\
By Master Theorem, complexity = $O(n^3)$


### Strassen's Algorithm
This implementation requires only 7 multiplications instead of 8, as we further divide corresponding subpart products into submodules consisting of additive/subtractive equations. As addition/subtraction is far more economical than multiplication, this is favorable.\
Recurrence relation: $T(n) = 7T(\frac{n}{2}) + O(n^2)$\
By Master Theorem, complexity = $O(n^{\log_2 7}) = O(n^{2.81})$

---

## The Median Problem

### Naive Approach
Since the problem is to simply output the median of $n$ numbers, the first-cut solution is naturally to perform mergesort on the set of numbers and output the middle-element.\
Complexity: $O(n logn)$

### Using Divide and Conquer
For any one number in the list, the list is separated into three parts, consisting of all the elements in the list that are *smaller, equal to, and larger* than the number, respectively.\
The search can instantly be narrowed down to one of these sublists, i.e. we can quickly determine which of them holds the median.\
We then recurse on the appropriate sublist. Thus, we can find the median in linear time, and the computations can even be performed in-place.\
Thus, Complexity = $O(n)$

---

